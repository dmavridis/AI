{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "P1.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQzwiqjhUaBf"
      },
      "source": [
        "# P1: Solve the OpenAI Gym [Taxi V3](https://gym.openai.com/envs/Taxi-v3/) Environment\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "glNI2UUWUaBk"
      },
      "source": [
        "## Introduction\n",
        "[OpenAI Gym](https://gym.openai.com/docs/) is a framework that provides RL environments of varying complexity with the same standard API making it easy to develop and benchmark RL algorithms. The [Taxi-V3](https://gym.openai.com/envs/Taxi-v3/) environmnet present a simple, text environment where actions and state (observations) are both discrete. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUVkHNjyUaBl"
      },
      "source": [
        "import gym"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rc89zgsKUaBm"
      },
      "source": [
        "The `gym.make()` API can be used to spawn any of the available environments by passing its full name."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wuhYKeT2UaBo"
      },
      "source": [
        "taxi = gym.make('Taxi-v3')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpEZEeaiUaBo"
      },
      "source": [
        "The Taxi environment has 500 states and 6 possible actions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YSTEC_KnUaBp",
        "outputId": "7771cd45-f40d-41fb-9616-8c6a76fa6ab1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "taxi.action_space"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Discrete(6)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CRulBfGUUaBq",
        "outputId": "f5ee796f-52c9-4af3-a1ee-acd07c3fb80a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "taxi.observation_space"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Discrete(500)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZ-0mvbVUaBq"
      },
      "source": [
        "The task and reward structure are described in the [documentation](https://github.com/openai/gym/blob/a5a6ae6bc0a5cfc0ff1ce9be723d59593c165022/gym/envs/toy_text/taxi.py#L25)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cA9p08BWUaBr",
        "outputId": "cfea7ac1-c08f-40a4-b2e4-d36b7218b0a4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "taxi.reset()\n",
        "taxi.render()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+\n",
            "|R: |\u001b[43m \u001b[0m: :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
            "+---------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gvi65Wv5UaBs"
      },
      "source": [
        "## Submission\n",
        "- Submit your solution as a Jupyter notebook. \n",
        "- Ensure that all cells in the notebook have been executed and the output is showing\n",
        "- Ensure that your solution consistently reaches the average cumulative reward defined in the rubric (link below)\n",
        "- Post your solution on Github and share the link to your commit as a direct message in Slack"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ymQKv9lUaBt"
      },
      "source": [
        "## Evaluation\n",
        "The goal of the project is to get a certain average (cumulative) reward over 100 episodes. To pass the project, you must meet all the requirments in the project [rubric](https://github.com/KnowchowHQ/rl-in-action/blob/master/C1-RL-Intro/W3OH/P1-rubric.md)"
      ]
    }
  ]
}