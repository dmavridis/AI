{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "P1.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQzwiqjhUaBf"
      },
      "source": [
        "# P1: Solve the OpenAI Gym [Taxi V3](https://gym.openai.com/envs/Taxi-v3/) Environment\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "glNI2UUWUaBk"
      },
      "source": [
        "## Introduction\n",
        "[OpenAI Gym](https://gym.openai.com/docs/) is a framework that provides RL environments of varying complexity with the same standard API making it easy to develop and benchmark RL algorithms. The [Taxi-V3](https://gym.openai.com/envs/Taxi-v3/) environmnet present a simple, text environment where actions and state (observations) are both discrete. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUVkHNjyUaBl"
      },
      "source": [
        "import gym"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rc89zgsKUaBm"
      },
      "source": [
        "The `gym.make()` API can be used to spawn any of the available environments by passing its full name."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wuhYKeT2UaBo"
      },
      "source": [
        "taxi = gym.make('Taxi-v3')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpEZEeaiUaBo"
      },
      "source": [
        "The Taxi environment has 500 states and 6 possible actions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YSTEC_KnUaBp",
        "outputId": "7771cd45-f40d-41fb-9616-8c6a76fa6ab1"
      },
      "source": [
        "taxi.action_space"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Discrete(6)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CRulBfGUUaBq",
        "outputId": "f5ee796f-52c9-4af3-a1ee-acd07c3fb80a"
      },
      "source": [
        "taxi.observation_space"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Discrete(500)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZ-0mvbVUaBq"
      },
      "source": [
        "The task and reward structure are described in the [documentation](https://github.com/openai/gym/blob/a5a6ae6bc0a5cfc0ff1ce9be723d59593c165022/gym/envs/toy_text/taxi.py#L25)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cA9p08BWUaBr",
        "outputId": "cfea7ac1-c08f-40a4-b2e4-d36b7218b0a4"
      },
      "source": [
        "taxi.reset()\n",
        "taxi.render()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+\n",
            "|R: |\u001b[43m \u001b[0m: :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
            "+---------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gvi65Wv5UaBs"
      },
      "source": [
        "## Submission\n",
        "- Submit your solution as a Jupyter notebook. \n",
        "- Ensure that all cells in the notebook have been executed and the output is showing\n",
        "- Ensure that your solution consistently reaches the average cumulative reward defined in the rubric (link below)\n",
        "- Post your solution on Github and share the link to your commit as a direct message in Slack"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ymQKv9lUaBt"
      },
      "source": [
        "## Evaluation\n",
        "The goal of the project is to get a certain average (cumulative) reward over 100 episodes. To pass the project, you must meet all the requirments in the project [rubric](https://github.com/KnowchowHQ/rl-in-action/blob/master/C1-RL-Intro/W3OH/P1-rubric.md)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sux6fh6a7yXq"
      },
      "source": [
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self, nA=6):\n",
        "        \"\"\" Initialize agent.\n",
        "        Params\n",
        "        ======\n",
        "        - nA: number of actions available to the agent\n",
        "        \"\"\"\n",
        "        self.nA = nA\n",
        "        self.Q = defaultdict(lambda: np.zeros(self.nA))\n",
        "        self.alpha = 0.1\n",
        "        self.gamma = .9\n",
        "        self.policy = np.zeros(self.nA)\n",
        "        self.epsilon = 0.9999\n",
        "\n",
        "    def epsilon_greedy_probs(self, Q_s):\n",
        "        \"\"\" obtains the action probabilities corresponding to epsilon-greedy policy \"\"\"\n",
        "        \n",
        "        self.epsilon = self.epsilon**2\n",
        "#        epsilon = (1/self.count)**0.6\n",
        "        policy_s = np.ones(self.nA) * self.epsilon / self.nA\n",
        "        policy_s[np.argmax(Q_s)] = 1 - self.epsilon + (self.epsilon / self.nA)\n",
        "        return policy_s    \n",
        "    \n",
        "    def select_action(self, state):\n",
        "        \"\"\" Given the state, select an action.\n",
        "        Params\n",
        "        ======\n",
        "        - state: the current state of the environment\n",
        "        Returns\n",
        "        =======\n",
        "        - action: an integer, compatible with the task's action space\n",
        "        \"\"\"\n",
        "        self.policy_s = self.epsilon_greedy_probs(self.Q[state])\n",
        "        return np.random.choice(np.arange(self.nA), p=self.policy_s)\n",
        "\n",
        "    def step(self, state, action, reward, next_state, done):\n",
        "        \"\"\" Update the agent's knowledge, using the most recently sampled tuple.\n",
        "        Params\n",
        "        ======\n",
        "        - state: the previous state of the environment\n",
        "        - action: the agent's previous choice of action\n",
        "        - reward: last reward received\n",
        "        - next_state: the current state of the environment\n",
        "        - done: whether the episode is complete (True or False)\n",
        "        \"\"\"\n",
        "        \n",
        "        # Implementation of SARSA max\n",
        "\n",
        "        # self.Q[state][action] = self.Q[state][action] + self.alpha*(reward + \\\n",
        "        #self.gamma * max(self.Q[next_state]) - self.Q[state][action])\n",
        "         \n",
        "        self.Q[state][action] = self.Q[state][action] + self.alpha*(reward + \\\n",
        "               self.gamma * np.dot(self.policy_s,self.Q[next_state]) - self.Q[state][action])      \n",
        "        \n",
        "        # next_action = np.random.choice(np.arange(self.nA), p=self.policy_s)\n",
        "        # self.Q[state][action] = self.Q[state][action] + self.alpha*(reward + \\\n",
        "        # self.gamma * self.Q[next_state][next_action] - self.Q[state][action])         "
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ep-SIDlY8eCd"
      },
      "source": [
        "from collections import deque\n",
        "import sys\n",
        "import math\n",
        "import numpy as np\n",
        "\n",
        "def interact(env, agent, num_episodes=20000, window=100):\n",
        "    \"\"\" Monitor agent's performance.\n",
        "    \n",
        "    Params\n",
        "    ======\n",
        "    - env: instance of OpenAI Gym's Taxi-v1 environment\n",
        "    - agent: instance of class Agent (see Agent.py for details)\n",
        "    - num_episodes: number of episodes of agent-environment interaction\n",
        "    - window: number of episodes to consider when calculating average rewards\n",
        "    Returns\n",
        "    =======\n",
        "    - avg_rewards: deque containing average rewards\n",
        "    - best_avg_reward: largest value in the avg_rewards deque\n",
        "    \"\"\"\n",
        "    # initialize average rewards\n",
        "    avg_rewards = deque(maxlen=num_episodes)\n",
        "    # initialize best average reward\n",
        "    best_avg_reward = -math.inf\n",
        "    # initialize monitor for most recent rewards\n",
        "    samp_rewards = deque(maxlen=window)\n",
        "    # for each episode\n",
        "    for i_episode in range(1, num_episodes+1):\n",
        "        # begin the episode\n",
        "        state = env.reset()\n",
        "        # initialize the sampled reward\n",
        "        samp_reward = 0\n",
        "        while True:\n",
        "            # agent selects an action\n",
        "            action = agent.select_action(state)\n",
        "            # agent performs the selected action\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            # agent performs internal updates based on sampled experience\n",
        "            agent.step(state, action, reward, next_state, done)\n",
        "            # update the sampled reward\n",
        "            samp_reward += reward\n",
        "            # update the state (s <- s') to next time step\n",
        "            state = next_state\n",
        "            if done:\n",
        "                # save final sampled reward\n",
        "                samp_rewards.append(samp_reward)\n",
        "                break\n",
        "        if (i_episode >= 100):\n",
        "            # get average reward from last 100 episodes\n",
        "            avg_reward = np.mean(samp_rewards)\n",
        "            # append to deque\n",
        "            avg_rewards.append(avg_reward)\n",
        "            # update best average reward\n",
        "            if avg_reward > best_avg_reward:\n",
        "                best_avg_reward = avg_reward\n",
        "        # monitor progress\n",
        "        print(\"\\rEpisode {}/{} || Best average reward {}\".format(i_episode, num_episodes, best_avg_reward), end=\"\")\n",
        "        sys.stdout.flush()\n",
        "        # check if task is solved (according to OpenAI Gym)\n",
        "        if best_avg_reward >= 9.7:\n",
        "            print('\\nEnvironment solved in {} episodes.'.format(i_episode), end=\"\")\n",
        "            break\n",
        "        if i_episode == num_episodes: print('\\n')\n",
        "    return avg_rewards, best_avg_reward"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOvf2INX8rOs",
        "outputId": "d545a359-6def-466d-e7be-36573deb2167",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "agent = Agent()\n",
        "avg_rewards, best_avg_reward = interact(taxi, agent)\n",
        "\n",
        "\n",
        "'''\n",
        " Best \n",
        "gamma = 0.1\n",
        "alpha = 0.1\n",
        "epsilon = (1/self.count)**(0.6)\n",
        "'''"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Episode 20000/20000 || Best average reward 8.8\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n Best \\ngamma = 0.1\\nalpha = 0.1\\nepsilon = (1/self.count)**(0.6)\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GE7l4SlI84xT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}