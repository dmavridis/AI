{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Neural Networks\n",
    "## Introduction\n",
    "- What is Deep Learning?\n",
    "- Where it is useful? : Everywhere\n",
    "\n",
    "## Classification Problems\n",
    "- How do we find the seperation line?\n",
    "\n",
    "## Linear Boundaries\n",
    "Make $ \\hat{y} $ as close as possible to $y $\n",
    "\n",
    "## Higher Dimensions\n",
    "n-dimensional space $x_1, x_2, ...$\n",
    "\n",
    "Boundary: n-1 dimensional hyperplane\n",
    "\n",
    "## Perceptrons\n",
    "Step Function\n",
    "\n",
    "Various logic functions\n",
    "\n",
    "## Perceptron algorithm\n",
    "1. Start with random weights $w_1, w_2, ..., w_n, b$\n",
    "2. For every misclassified point $(x_1, x_2, ..., x_n)$ :\n",
    "  - If prediction = 0\n",
    "    - For $i=1 ..n$\n",
    "      - Change $w_i + a x_i$\n",
    "    - Change b to b+a  \n",
    "  - If prediction = 1\n",
    "    - For $i=1 ..n$\n",
    "      - Change $w_i - a x_i$\n",
    "    - Change b to b-a  \n",
    "    \n",
    "## Non - Linear data\n",
    "Error - Function = Distance\n",
    "\n",
    "Error function should be ontinuous to avoid getting stuck in local minima. Small variations will make difference\n",
    "\n",
    "## Log - loss error function\n",
    "\n",
    "To move from discrete to continuous probability predictions, we change the activation function from step to sigmoid. \n",
    "\n",
    "$$ \\sigma(x) = \\frac{1}{1+e^{-x}} $$\n",
    "\n",
    "$$ \\hat{y} = \\sigma(Wx+b)$$\n",
    "\n",
    "## Softmax\n",
    "Use of exponential\n",
    "\n",
    "## One-hot encoding\n",
    "\n",
    "## Maximum Likelihood\n",
    "Purpose is to maximize probabilities\n",
    "## Cross Entropy\n",
    "A good model has a low cross entropy\n",
    "\n",
    "**Goal:** Minimize cross entropy\n",
    "\n",
    "$$Cross-Entropy = -\\sum_{i=1}^{n}[y_iln(p_i) + (1-y_i)ln(1-p_i)]$$\n",
    "\n",
    "### Multiclass cross entropy\n",
    "$$Cross-Entropy = -\\sum_{i=1}^{n}\\sum_{j=1}^{m}y_{ij}ln(p_{ij})$$\n",
    "\n",
    "## Logistic Regression\n",
    "\n",
    "$$Error Function = -\\frac{1}{m}\\Sigma_{i=1}^{m}[(1-y_i)ln(1-\\hat{y_i})+y_iln(\\hat{y_i})] $$\n",
    "$$E(W,b) = -\\frac{1}{m}\\Sigma_{i=1}^{m}(1-y_i)ln(1-\\sigma{(Wx^{(i)}+b)}) + y_iln(\\sigma{(Wx^{(i)}+b}))$$\n",
    "\n",
    "Goal is to minimize the Error\n",
    "\n",
    "## Gradient Descent\n",
    "$$\\sigma'{(x)} $$\n",
    "\n",
    "\n",
    "The gradient is actually a scalar times the coordinates of the point! And what is the scalar? Nothing less that the difference between the label and the prediction. That means, if the label is close to the prediction (meaning the point is well classified), this gradient is small, and if the label is far from the prediction (meaning the point is poorly classified), then this gradient is large. Let's remember, a small gradient means we'll change our coordinates by a little bit, and a large gradient means we'll change our coordinates by a lot.\n",
    "\n",
    "## Logistic Regression Algorithm\n",
    "\n",
    "1. Start with random weights: $w_1, ..., w_n, b$\n",
    "2. For every point $x_1, ..., x_n$ :\n",
    "  \n",
    "  2.1. For $ i = 1 ... n$\n",
    "  \n",
    "    2.1.1. Update $w_i'$\n",
    "    \n",
    "    2.1.2. Update $b'$\n",
    "    \n",
    "3. Repeat until error is minimum\n",
    "\n",
    "## Perceptron vs Gradient Descent\n",
    "\n",
    "Percepton error is +-1 if misclassified\n",
    "\n",
    "In Perceptron when correctly classified, it tell the line to do nothing. Gradient descent tells the line to go away\n",
    "\n",
    "## Non-linear Models\n",
    "\n",
    "Get a linear combination of two linear models\n",
    "\n",
    "## Feedforward\n",
    "Training means what parameters they should have at the edges in order to model the data well\n",
    "\n",
    "## Categorical cross entropy\n",
    "\n",
    "## Local Connectivity\n",
    "MLP's \n",
    "- Only use **fully** connected layers \n",
    "- Only accept vectors as inputs\n",
    "\n",
    "CNN's\n",
    "- Also use **sparsely** connected layers\n",
    "- Also accept matrices as inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
